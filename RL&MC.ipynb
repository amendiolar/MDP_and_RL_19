{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL&MC.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9iC6YkZ4EQ6",
        "colab_type": "text"
      },
      "source": [
        "**Cadenas de Markov**\n",
        "\n",
        "Una cadena de Markov (CM) es un tipo particular de proceso estocástico.\n",
        "Una CM tiene probabilidades de transición estacionarias, es decir, no dependen del tiempo. Las probabilidades del estado siguiente dependen únicamente del estado presente y no de los estados pasados. Esto permite que la CM se pueda representar de manera compacta con un grafo.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1K4yyA6Whp88NVc2ocznvcjevvauGCMg5)\n",
        "\n",
        "Por ejemplo:\n",
        "\n",
        "En este caso la CM tiene dos estados.\n",
        "Los parámetros de las aristas son las probabilidades de transición de estado.\n",
        "Por ejemplo, en esta CM la probabilidad de transitar del estado 1 al estado 2 es de 0.7. \n",
        "\n",
        "Esto lo denotamos como $P(s^{t+1}{=}2 \\mid s^{t}{=}1) = 0.7$\n",
        "\n",
        "Las probabilidades de transición de una cadena de Markov se pueden representar de manera matricial.\n",
        "\n",
        "En este caso la matrix de probabilidades de transición $T$ queda dada por: \n",
        "\n",
        "$T = \n",
        "\\begin{bmatrix}\n",
        "0.3&0.6 \\\\\n",
        "0.7&0.4\n",
        "\\end{bmatrix}\n",
        "$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rApv0Q5x4CgX",
        "colab_type": "code",
        "outputId": "cc8454c6-b0c3-41c1-fa68-59a5c2fdeb14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np #Importación de la librería NumPy, Numerical Python empleado para el manejo de arreglos multidimesionales\n",
        "\n",
        "T = np.matrix([[0.3,0.6],[0.7,0.4]]) #Creación de la matriz 2x2 T\n",
        "\n",
        "print(T) #Imprimir T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.3 0.6]\n",
            " [0.7 0.4]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APoaNG5Q8JQR",
        "colab_type": "text"
      },
      "source": [
        "La evolución de la distribución de probabilidad $x$ para los dos estados esta dada por la ecuación:\n",
        "\n",
        "\n",
        "$\n",
        " x = T x\n",
        "$\n",
        " \n",
        "Por ejemplo, si sabemos con certeza que estamos en el estado 1, se representa como:\n",
        " \n",
        "$\n",
        "x = \n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "0\n",
        "\\end{bmatrix}\n",
        "$\n",
        " \n",
        "Es decir la probabilidad de estar en el estado 1 es 1 y de estar en el estado 2 es 0.\n",
        " \n",
        "Si queremos saber como será la distribución de probabilidad para los estados en el tiempo siguiente hacemos:\n",
        "\n",
        "$x^{t+1} = \n",
        "\\begin{bmatrix}\n",
        "0.3&0.6\\\\\n",
        "0.7&0.4\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "0\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.3\\\\\n",
        "0.7\n",
        "\\end{bmatrix}\n",
        "$\n",
        " \n",
        "Es decir la probabilidad de estar en el estado 1 es 0.3 y de transitar al estado 2 es 0.7. \n",
        "Si queremos saber que pasará dos tiempos en el futuro, iteramos la ecuación:\n",
        "\n",
        "$x^{t+2} = \n",
        "\\begin{bmatrix}\n",
        "0.3&0.6\\\\\n",
        "0.7&0.4\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "0.3&0.6\\\\\n",
        "0.7&0.4\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "0\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "0.3&0.6\\\\\n",
        "0.7&0.4\n",
        "\\end{bmatrix}^2\n",
        "\\begin{bmatrix}\n",
        "1\\\\\n",
        "0\n",
        "\\end{bmatrix}\n",
        "$\n",
        " \n",
        "Con ayuda de NumPy podemos resolver esta multiplicación de matrices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVMd0z_j4DVc",
        "colab_type": "code",
        "outputId": "14240942-7ec7-41d2-ef5a-c9454dcfded3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "x = np.matrix([[1],[0]]) #Probabilidades de estados\n",
        "print('x =\\n'+str(x)+'\\n')\n",
        "\n",
        "x_1 = np.matmul(T,x) #Multiplicación de la matriz de estados por la matriz de probabilidades de transición usando la función matmul() de NumPy.\n",
        "print('x(t+1)=\\n'+str(x_1)+'\\n')\n",
        "\n",
        "x_2 = np.matmul(T,x_1) #Multiplicación para calcular la probabilidad de estados a dos pasos de tiempo\n",
        "print('x(t+2)=\\n'+str(x_2)+'\\n')\n",
        "\n",
        "#Comprobar que el resultado anterior es el mismo que elevar la matriz $T$ al cuadrado y multiplicar por $x$ usando la función matrix_power() de NumPy.\n",
        "x_2= np.matmul(np.linalg.matrix_power(T,2),x)\n",
        "print('x(t+2)=\\n'+str(x_2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x =\n",
            "[[1]\n",
            " [0]]\n",
            "\n",
            "x(t+1)=\n",
            "[[0.3]\n",
            " [0.7]]\n",
            "\n",
            "x(t+2)=\n",
            "[[0.51]\n",
            " [0.49]]\n",
            "\n",
            "x(t+2)=\n",
            "[[0.51]\n",
            " [0.49]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o2mQ32mKA07",
        "colab_type": "text"
      },
      "source": [
        "Bajo ciertas condiciones, existe una distribución de probabilidad estacionaria sobre los estados.\n",
        "Esto significa que no importa cual sea la distribución de probabilidad inicial, al iterar la ecuación, la distribución converge a una distribución que no cambia. \n",
        "\n",
        "Por ejemplo, definamos un método calcula_dist() para encontrar la distribución de probabilidad al tiempo t ó $n$, dada una condición inicial $x_0$, empleando la misma matriz de probabilidad de transición de estados $T$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ttqw1x55LE11",
        "colab_type": "code",
        "outputId": "7e6843dd-da67-46c0-ba97-d9afc1cd70ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "def calcula_dist(T,x_0,n):\n",
        "    x_n = np.matmul(np.linalg.matrix_power(T,n),x_0)\n",
        "    print('x(t+'+str(n)+')=\\n'+str(x_n))\n",
        "\n",
        "\n",
        "#Usemos esta función para encontrar la distribución al tiempo 5:\n",
        "x_10 = calcula_dist(T,x,10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x(t+10)=\n",
            "[[0.46154164]\n",
            " [0.53845836]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1JkMqKbLMJE",
        "colab_type": "code",
        "outputId": "2899d7a1-78bf-4aa5-c5b6-5d18d93e47db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 885
        }
      },
      "source": [
        "# Veamos como cambia la distribución con cada una de las iteraciones:\n",
        "for i in range(10):\n",
        "    calcula_dist(T,x,i)\n",
        "    print(\"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x(t+0)=\n",
            "[[1.]\n",
            " [0.]]\n",
            "\n",
            "\n",
            "x(t+1)=\n",
            "[[0.3]\n",
            " [0.7]]\n",
            "\n",
            "\n",
            "x(t+2)=\n",
            "[[0.51]\n",
            " [0.49]]\n",
            "\n",
            "\n",
            "x(t+3)=\n",
            "[[0.447]\n",
            " [0.553]]\n",
            "\n",
            "\n",
            "x(t+4)=\n",
            "[[0.4659]\n",
            " [0.5341]]\n",
            "\n",
            "\n",
            "x(t+5)=\n",
            "[[0.46023]\n",
            " [0.53977]]\n",
            "\n",
            "\n",
            "x(t+6)=\n",
            "[[0.461931]\n",
            " [0.538069]]\n",
            "\n",
            "\n",
            "x(t+7)=\n",
            "[[0.4614207]\n",
            " [0.5385793]]\n",
            "\n",
            "\n",
            "x(t+8)=\n",
            "[[0.46157379]\n",
            " [0.53842621]]\n",
            "\n",
            "\n",
            "x(t+9)=\n",
            "[[0.46152786]\n",
            " [0.53847214]]\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxyTo1VXLFLR",
        "colab_type": "text"
      },
      "source": [
        "Observamos una rápida convergencia a los valores.\n",
        "¿Qué tanto tenemos que iterar la ecuación para encontrar la distribución?\n",
        "Esto puede resolverse más fácilmente si observamos que lo que queremos encontrar es $x^{\\text{*}}$ tal que:\n",
        " \n",
        "$x^{\\text{*}} = T x^{\\text{*}}$\n",
        "\n",
        "Esto es equivalente a resolver el problema de encontrar los vectores característicos de la matriz $T$.\n",
        "\n",
        "Los vectores característicos de una matriz, también conocidos como eigenvectores,  son aquellos que no cambian de dirección cuando se aplica la transformación lineal $T$ (es decir cuando se multiplica por $T$).\n",
        "\n",
        "Usando NumPy podemos encontrar el eigenvector de una matriz, usando la función linalg.eig():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irc7ckFYMDd9",
        "colab_type": "code",
        "outputId": "444c1bda-7492-4c41-a030-a1905bf9be31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "#La función linalg.eg() regresa un tupple con un vector y una matriz. El vector corresponde a los eigenevalores y la matriz a los eigenvectores\n",
        "\n",
        "l,v = np.linalg.eig(T)\n",
        "print(\"Eigenvalores de matriz T:\\n\"+str(l))\n",
        "print(\"\\n\")\n",
        "print(\"Eigenvectores de matriz T:\\n\"+str(v)) #Eigenvectores de matriz T"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Eigenvalores de matriz T:\n",
            "[-0.3  1. ]\n",
            "\n",
            "\n",
            "Eigenvectores de matriz T:\n",
            "[[-0.70710678 -0.65079137]\n",
            " [ 0.70710678 -0.7592566 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UQb_FVnMTDQ",
        "colab_type": "code",
        "outputId": "82b2f25c-dfbe-46d5-bb94-0c2b175d7577",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Nos interesa el eigenvector que corresponde con el eigenvalor 1, ya que ambos valores del eigenvector (-0.651,-0.759) tienen el mimso signo. \n",
        "#En este caso el vector que esta como segunda columna en v. Lo extraemos y normalizamos:\n",
        "x_s = v[:,1] / sum(v[:,1])\n",
        "print(x_s)\n",
        "\n",
        "#La matriz de probabilidad de estados es la misma que la obtenida con la iteración a 10 pasos de la función calcula_dist(T,x_0,n)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.46153846]\n",
            " [0.53846154]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uvYEhIVR7as",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "**PRÁCTICA Considera la CM representada por el siguiente diagrama:**\n",
        "\n",
        "![Ejercicio práctico](https://drive.google.com/uc?export=view&id=1rmq02Ip_pJ_vFveXAA4wLOH9pmoGlb50)\n",
        "\n",
        "**Encuentra la distribución de probabilidad estacionaria.**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PYSj5UpV2Uk",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**Procesos de Decisión de Markov (MDPs o PDMs)**\n",
        "\n",
        "En los Procesos de Decisión de Markov (PDMs) encadenamos la decisiones del agente con un proceso estocástico. \n",
        "Nos interesa resolver la ecuación de Bellman para la política del agente.\n",
        "Recordemos que la política le dice al agente que acción tomar en cada estado posible.\n",
        "\n",
        "Para encontrar la politica optima $\\pi^*$ podemos usar el algoritmo de iteración de políticas.\n",
        "A continuación te damos una implementación básica del algoritmo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qa5xFMcAWENn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#Itertools es una librería con funciones para crear bucles (loops) eficientes. \n",
        "#La función product() regresa el producto cartesiano de dos vectores input, equivalente a un bucle anidado (nested for-loop) \n",
        "from itertools import product \n",
        "\n",
        "#La función choice() de la librería random elige un elemento al azar de una lista input. \n",
        "from random import choice \n",
        "\n",
        "class MDP: #Classes son instancias de Python que actúan como constructores de objetos\n",
        "    \n",
        "    def __init__(self,s,r,a,T,gamma): #Cualquier instancia class, inicia con una función __init__() para asignar valores a las propiedades de los objectos.\n",
        "        \"\"\"\n",
        "        Builds the MDP problem\n",
        "        :param s: states\n",
        "        :param r: rewards\n",
        "        :param a: actions\n",
        "        :param T: dictionary where keys are (s,a) pairs and\n",
        "        values are probabilities\n",
        "        :param gamma: the discount factor\n",
        "        \"\"\"\n",
        "        self.s = s\n",
        "        self.r = r\n",
        "        self.a = a\n",
        "        self.T = T\n",
        "        self.gamma = gamma\n",
        "        \n",
        "    def policy_iteration(self,pi=None):\n",
        "        \"\"\"\n",
        "            Policy iteration algorithm\n",
        "        \"\"\"\n",
        "        #initial random policy\n",
        "        if not pi:\n",
        "            pi = [choice(self.a) for s in self.s]\n",
        "        \n",
        "        print('pi = '+str(pi))\n",
        "        self.obtainT(pi);\n",
        "        T = self.obtainT(pi)\n",
        "        print('T(pi) = \\n'+str(T))\n",
        "        \n",
        "        V = np.matmul(np.linalg.inv(np.eye(3)-self.gamma*T.T),self.r)\n",
        "        print('V(s) = '+ str(V))\n",
        "        \n",
        "        pi_star = self.find_pi_star(V)\n",
        "        print(\"pi*(s) = \"+str(pi_star))\n",
        "        \n",
        "        if pi_star == pi:\n",
        "            return pi\n",
        "        else:\n",
        "            return self.policy_iteration(pi_star)\n",
        "        \n",
        "    def obtainT(self,pi):\n",
        "        \"\"\"\n",
        "        Obtains the transition probability matrix parametrized by the policy pi\n",
        "        :param pi: the policy\n",
        "        \"\"\"\n",
        "        return           np.matrix([[self.T[(s,t,pi[s])] for s in self.s] for t in self.s])\n",
        "        \n",
        "    def find_pi_star(self,V):\n",
        "        \"\"\"\n",
        "        Finds the optimal policy for the given infinite horizon values\n",
        "        :param V: the infinite horizon expected utility\n",
        "        \"\"\"\n",
        "        \n",
        "        print(\"\\n\".join([str((x,np.matmul(self.obtainT(x).T,np.array(V).T))) for x in product(self.a,self.a,self.a)]))\n",
        "        \n",
        "        return list(max(product(self.a,self.a,self.a),            key=lambda x: np.sum(np.matmul(self.obtainT(x).T,np.array(V).T))))\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thf_mvzQWEoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Para ejecutar el algoritmo primero definimos los estados posibles:\n",
        "estados = [0,1,2]\n",
        "print(estados)\n",
        "\n",
        "# Las acciones posibles:\n",
        "acciones = [0,1]\n",
        "print(acciones)\n",
        "\n",
        "# Ahora las recompensas para cada estado:\n",
        "recompensas = [0,10,27]\n",
        "print(recompensas)\n",
        "\n",
        "# El parámetro gamma es el factor de descuento para la utilidad de valores futuros:\n",
        "gamma = 0.9\n",
        "print(gamma)\n",
        "\n",
        "\n",
        "# Ahora representamos las probabilidaddes condicionales con un diccionario.\n",
        "# <img src=\"mdpejemplo2.png\" alt=\"mdp\" width=\"914\"/>\n",
        "\n",
        "# La llave será una tupla con tres elementos, los primeros dos contienen los índices de la transición de estado, el último es la acción que se toma.\n",
        " \n",
        "# El valor del diccionario es la probabilidad asociada al estado, acción correspondiente.\n",
        "\n",
        "T={\n",
        "    (0,0,0):0.7,(0,0,1):0.5, (1,0,0):0.4,(1,0,1):0.2, (2,0,0):0.2,(2,0,1):0.1,\n",
        "    (0,1,0):0.1,(0,1,1):0.3, (1,1,0):0.4,(1,1,1):0.7, (2,1,0):0.2,(2,1,1):0.1,\n",
        "    (0,2,0):0.2,(0,2,1):0.2, (1,2,0):0.2,(1,2,1):0.1, (2,2,0):0.6,(2,2,1):0.8\n",
        "}\n",
        "print(T)\n",
        "\n",
        "\n",
        "# Vamos a crear una instancia del PDM:\n",
        "mdp = MDP(estados,recompensas,acciones,T,gamma)\n",
        "\n",
        "\n",
        "# Comprobamos que obtenemos la misma política que el ejemplo del tren inteligente.\n",
        "# \n",
        "# En el ejemplo $\\pi_0 = \\begin{bmatrix}0&0&0\\end{bmatrix}$.\n",
        "\n",
        "\n",
        "pi_0 = [0,0,0]\n",
        "print(\"T(pi_0) = \\n\"+str(mdp.obtainT(pi_0)))\n",
        "\n",
        "\n",
        "# Ahora invocamos el algoritmo de iteración de políticas.\n",
        "# \n",
        "# El algoritmo imprime como cambia la política con las iteraciones.\n",
        "\n",
        "politica = mdp.policy_iteration(pi_0)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hyX0tf8ZLY1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "---\n",
        "\n",
        "**PRÁCTICA**\n",
        "\n",
        "**Si cambiamos el valor las recompensas a $[0, 20, 20]$, ¿cambia la politica?**\n",
        "\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAKByEmZZ5JW",
        "colab_type": "text"
      },
      "source": [
        "**Reinforcement Learning: MDPs and Q-Learning**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_O7z-Q8Z5dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "\n",
        "@author: stan\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pylab as pl\n",
        "from IPython import display\n",
        "from itertools import product\n",
        "from math import inf,sqrt\n",
        "from random import choice,choices\n",
        "\n",
        "INACCESIBLE = 0\n",
        "ACCESIBLE = 1\n",
        "TERMINAL = 2\n",
        "\n",
        "class Map:\n",
        "    \n",
        "    def __init__(self,types,rewards):\n",
        "        self.types = np.matrix(types)\n",
        "        self.rewards = np.matrix(rewards)\n",
        "        self.rows,self.cols = self.types.shape \n",
        "        #padding for plots\n",
        "        self.e = 0.04\n",
        "        x = 1/sqrt(2)\n",
        "        #arrows per action\n",
        "        self.arrows = {'N':(0,1),\n",
        "                       'NE':(x,x),\n",
        "                       'E':(1,0),\n",
        "                       'SE':(x,-x),\n",
        "                       'S':(0,-1),\n",
        "                       'SW':(-x,-x),\n",
        "                       'W':(-1,0),\n",
        "                       'NW':(-x,x)}\n",
        "\n",
        "        self.point = None\n",
        "        \n",
        "    def __str__(self):\n",
        "        return 'types: '+'\\n'+str(self.types)+'\\n'+\\\n",
        "                'rewards: '+'\\n'+str(self.rewards)\n",
        "        \n",
        "    def __repr__(self):\n",
        "        return str(self)\n",
        "    \n",
        "    def display_map(self):\n",
        "        pl.figure()\n",
        "        ax = pl.gca()\n",
        "        ax.axis(\"equal\")\n",
        "        xcoor = [0,0,1,1,0]\n",
        "        ycoor = [0,1,1,0,0]\n",
        "        colors = ['black','green','red']\n",
        "        e = self.e\n",
        "        for i in range(self.rows):\n",
        "            r = [x+(i*(e+1)) for x in xcoor]\n",
        "            for j in range(self.cols):\n",
        "                c = [y+(j*(e+1)) for y in ycoor]\n",
        "                pl.fill(c,r,colors[self.types[i,j]])\n",
        "        #check this on jupyter notebooks\n",
        "        #display.display(pl.gcf())\n",
        "        pl.axis('off')\n",
        "        pl.title('Agent environment')\n",
        "        \n",
        "    def display_rewards(self):\n",
        "        pl.figure()\n",
        "        cmap = pl.cm.bwr\n",
        "        cmap.set_bad(color='black')\n",
        "        r = np.ma.masked_where(self.rewards==0.0,self.rewards)\n",
        "        pl.imshow(r,cmap=cmap,origin='lower')\n",
        "        pl.colorbar()\n",
        "        pl.axis('off')\n",
        "        pl.title('Reward signal')\n",
        "        \n",
        "    def animate_Vs(self,V, error, reset=False, delay=0.5):\n",
        "        if reset:\n",
        "            pl.figure()\n",
        "        else:\n",
        "            self.cb.remove()\n",
        "        cmap = pl.cm.bwr\n",
        "        cmap.set_bad(color='black')\n",
        "        vs= self.rewards.copy()\n",
        "        for (x,y),v in V.items():\n",
        "            vs[y,x]=v\n",
        "        r = np.ma.masked_where(self.rewards==0.0,vs)\n",
        "        pl.imshow(r,cmap=cmap,origin='lower')\n",
        "        pl.axis('off')\n",
        "        pl.title('Infinite horizon value function. Error = %f'%error)  \n",
        "        self.cb = pl.colorbar()\n",
        "        pl.pause(delay)\n",
        "\n",
        "        \n",
        "    def display_probs(self,probs):\n",
        "        pl.figure()\n",
        "        vbar = np.ones((3,1))\n",
        "        hbar = np.ones((1,11))\n",
        "        mat = np.vstack((\n",
        "                np.hstack((probs['NW'],vbar,probs['N'],vbar,probs['NE'])),\n",
        "                hbar,\n",
        "                np.hstack((probs['W'],vbar,np.zeros((3,3)),vbar,probs['E'])),\n",
        "                hbar,\n",
        "                np.hstack((probs['SW'],vbar,probs['S'],vbar,probs['SE']))))\n",
        "        r = np.ma.masked_where(mat==1,mat)\n",
        "        cmap = pl.cm.jet\n",
        "        cmap.set_bad(color='white')\n",
        "        pl.imshow(r,cmap=cmap)\n",
        "        pl.colorbar()\n",
        "        pl.axis('off')\n",
        "        pl.title('Stochastic move probabilities')\n",
        "    \n",
        "    def show(self,probs):\n",
        "        self.display_map()\n",
        "        self.display_rewards()\n",
        "        self.display_probs(probs)\n",
        "        \n",
        "    def animate_move(self,x_prev,x,reset=False,cumreward=None,delay=1):\n",
        "        #padding\n",
        "        e = self.e\n",
        "        if reset:\n",
        "            pl.figure()\n",
        "            self.point = False\n",
        "            ax = pl.gca()\n",
        "            ax.axis(\"equal\")\n",
        "            xcoor = [0,0,1,1,0]\n",
        "            ycoor = [0,1,1,0,0]\n",
        "            colors = ['black','green','red']\n",
        "            for i in range(self.rows):\n",
        "                r = [x+(i*(e+1)) for x in xcoor]\n",
        "                for j in range(self.cols):\n",
        "                    c = [y+(j*(e+1)) for y in ycoor]\n",
        "                    pl.fill(c,r,colors[self.types[i,j]])\n",
        "            display.display(pl.gcf())\n",
        "        x_prev = list(map(lambda c:c+0.5+e*c,x_prev))\n",
        "        x = list(map(lambda c:c+0.5+e*c,x))\n",
        "        pl.plot(*zip(x_prev,x),color='white')\n",
        "        if self.point:\n",
        "            ax = pl.gca()\n",
        "            ax.lines[-2].remove()\n",
        "        else:\n",
        "            self.point = True\n",
        "        pl.plot([x[0],x[0]],[x[1],x[1]],marker='.',c='cyan',markersize=40)    \n",
        "        if cumreward:\n",
        "            pl.title(\"Cummulative reward: \"+f'{cumreward:06.2f}')\n",
        "        pl.pause(delay)\n",
        "        \n",
        "    def show_policy(self,pi):\n",
        "        e = 2*self.e\n",
        "        pl.figure()\n",
        "        ax = pl.gca()\n",
        "        ax.axis(\"equal\")\n",
        "        xcoor = [0,0,1,1,0]\n",
        "        ycoor = [0,1,1,0,0]\n",
        "        colors = ['black','green','red']\n",
        "        for i in range(self.rows):\n",
        "            r = [x+(i*(2*e+1)) for x in xcoor]\n",
        "            for j in range(self.cols):\n",
        "                c = [y+(j*(2*e+1)) for y in ycoor]\n",
        "                pl.plot(c,r,colors[self.types[i,j]])\n",
        "                if self.types[(i,j)]== ACCESIBLE:\n",
        "                    ar\n",
        "                    row = self.arrows[pi[(j,i)]]\n",
        "                    ax.quiver(\n",
        "                            j*(2*e+1)+0.5,\n",
        "                            i*(2*e+1)+0.5,\n",
        "                            arrow[0],\n",
        "                            arrow[1],\n",
        "                            scale=2,\n",
        "                            scale_units='x')\n",
        "        pl.show()\n",
        "        display.display(pl.gcf())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Iw59gzuJqjl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MDPWorld:\n",
        "    \n",
        "    def __init__(self,filename):\n",
        "\n",
        "        self.types = {INACCESIBLE:'Inaccesible',\n",
        "                      ACCESIBLE:'Accesible',\n",
        "                      TERMINAL:'Terminal'}\n",
        "        self.actions = ['N','NE','E','SE','S','SW','W','NW']\n",
        "        self.n_actions = len(self.actions)\n",
        "        self.read(filename)\n",
        "        \n",
        "    def read(self,filename):\n",
        "        self.filename = filename\n",
        "        with open(filename) as f:\n",
        "            lines = [l.strip() for l in f.readlines()]\n",
        "        rows,cols = map(int,lines[0].split())\n",
        "        map_info = [l.split() for l in lines[1:rows+1]]\n",
        "        types = [list(map(int,row[0::2])) for row in map_info]\n",
        "        rewards = [list(map(float,row[1::2])) for row in map_info]\n",
        "        self.probs = {}\n",
        "        for i in range(0,self.n_actions):\n",
        "            probs_info = [l.split() for l in lines[rows+3*i+1:rows+3*(i+1)+1]]\n",
        "            self.probs[self.actions[i]]=np.matrix(\n",
        "                    [list(map(float,row)) for row in probs_info])\n",
        "        self.map = Map(types,rewards)\n",
        "        \n",
        "    def attempt_location(self,x_prev, x_new):\n",
        "        # returns the new location if it is an allowed move and\n",
        "        # the previous otherwise\n",
        "        return \\\n",
        "            x_new if 0<=x_new[0]<self.map.cols and 0<=x_new[1]<self.map.rows\\\n",
        "            and self.map.types[(x_new[1],x_new[0])] != INACCESIBLE else x_prev\n",
        "\n",
        "    \n",
        "    def reachable_state_probability_pairs(self,s,action):\n",
        "        probs = self.probs[action]\n",
        "        #state probability pairs\n",
        "        sp = {}\n",
        "        for x in product([-1,0,1],[-1,0,1]):\n",
        "            pos = self.attempt_location(s,(x[0]+s[0],x[1]+s[1]))\n",
        "            prob = probs[::-1,:][(x[1]+1,x[0]+1)]\n",
        "            if prob > 0:\n",
        "                sp[pos] = sp[pos] + prob if pos in sp else prob\n",
        "        return sp\n",
        "    \n",
        "    def move(self,s,action):\n",
        "        #thresholds for stochastic simulation\n",
        "        ths = np.cumsum(self.probs[action][::-1,:]).reshape(3,3)\n",
        "        r = np.random.uniform()\n",
        "        m = np.where(r<=ths)\n",
        "        pos = min(zip(m[0],m[1]))\n",
        "        #changed index order\n",
        "        s_p = self.attempt_location(\n",
        "            s,(s[0]+int(pos[1])-1,s[1]+int(pos[0])-1))\n",
        "        return (s_p,self.reward_of_cell(s_p))\n",
        "        \n",
        "    def simulate(self,pi,n,x=(0,0)):\n",
        "        self.map.animate_move(x,x,reset=True)\n",
        "        #rows and columns are swaped in the matrix\n",
        "        cumreward = self.reward_of_cell(x)\n",
        "        for i in range(n):\n",
        "            x_prev = x\n",
        "            x,reward = self.move(x,pi[x])\n",
        "            cumreward += reward\n",
        "            self.map.animate_move(x_prev,x,cumreward=cumreward)\n",
        "            if self.terminal_cell(x):\n",
        "                break\n",
        "        return cumreward\n",
        "            \n",
        "    def random_pi(self):\n",
        "        pi = {p:a for p,a in \\\n",
        "              zip(product(range(self.map.cols),range(self.map.rows)),\n",
        "              np.random.choice(self.actions,self.map.rows*self.map.cols))}\n",
        "        return pi\n",
        "    \n",
        "    def expected_reward(self,cell, action,V):\n",
        "        return sum(map(lambda i:V[i[0]]*i[1],\n",
        "            self.reachable_state_probability_pairs(cell,action).items()))\n",
        "        \n",
        "    def accesible_cell(self, cell):\n",
        "        return self.map.types [(cell[1],cell[0])] == ACCESIBLE\n",
        "    \n",
        "    def terminal_cell(self,cell):\n",
        "        return self.map.types [(cell[1],cell[0])] == TERMINAL\n",
        "    \n",
        "    def reward_of_cell(self,cell):\n",
        "        return self.map.rewards[(cell[1],cell[0])]\n",
        "    \n",
        "    def compute_v(self,gamma,animate=False,epsilon=1.e-9):\n",
        "        error = inf\n",
        "            \n",
        "        V = {cell:0.0 \\\n",
        "             if not self.terminal_cell(cell) else self.reward_of_cell(cell) \\\n",
        "             for cell in product(range(self.map.cols),range(self.map.rows))}\n",
        "        if animate == True:\n",
        "            self.map.animate_Vs(V,error,reset=True)\n",
        "        while error > epsilon*(1-gamma)/gamma:\n",
        "            V_prev = V.copy()\n",
        "            for cell in filter(lambda x:self.accesible_cell(x),V.keys()):\n",
        "                V[cell] = self.reward_of_cell(cell)+ \\\n",
        "                gamma*max([self.expected_reward(cell,action,V) \\\n",
        "                     for action in self.actions])\n",
        "            error = max([abs(V[k]-V_prev[k]) for k in V.keys()])\n",
        "            if animate:\n",
        "                self.map.animate_Vs(V,error)\n",
        "        return V\n",
        "                \n",
        "    def compute_optimal_pi(self,gamma,epsilon=1.e-9):\n",
        "        Vs = self.compute_v(gamma,animate=False,epsilon=epsilon)\n",
        "        pi = {}\n",
        "        for cell,v in Vs.items():\n",
        "                pi[cell] = max([(action,self.expected_reward(cell,action,Vs))\\\n",
        "                     for action in self.actions],key=lambda c:c[1])[0]       \n",
        "        return pi     \n",
        "    \n",
        "    def best_action_Q(self,Q,s):\n",
        "        return max([(a,Q[(s,a)]) for a in self.actions ],\n",
        "                    key=lambda x:x[1])[0]\n",
        "        \n",
        "    def q_learn(self,alpha,gamma,episodes,steps):\n",
        "        n,m = self.map.cols,self.map.rows\n",
        "        cells = [x for x in product(range(n),range(m))\\\n",
        "                 if self.accesible_cell(x) or self.terminal_cell(x)]\n",
        "        #initialize with instantaneous reward\n",
        "        self.Q = {(s,a):self.reward_of_cell(s) \\\n",
        "                  for s,a in product(cells,self.actions)}\n",
        "        for episode,s in enumerate(choices(cells,k=episodes),1):\n",
        "            # instantaneous reward\n",
        "            r = self.reward_of_cell(s)\n",
        "            for step in range(steps):\n",
        "                # explore action\n",
        "                a = choice(self.actions)\n",
        "                # attempt to move\n",
        "                s_p, r_p = self.move(s,a)\n",
        "                self.Q[(s,a)] = (1-alpha)*self.Q[(s,a)]+\\\n",
        "                alpha*(r+gamma*max([self.Q[(s_p,a)] for a in self.actions]))\n",
        "                s,r = s_p,r_p\n",
        "                if self.terminal_cell(s):\n",
        "                    # early termination\n",
        "                    break\n",
        "        return {s:self.best_action_Q(self.Q,s) for s in cells}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGVXRy4kS5Gr",
        "colab_type": "code",
        "outputId": "9d9fe211-98da-4fc4-efb2-d982f8795134",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Visualizar los datos en el documento problem-1.mdp\n",
        "import pandas as pd\n",
        "url = \"https://raw.githubusercontent.com/riiaa/MDP_and_RL/master/problem-2.mdp\"\n",
        "m = pd.read_csv(url)\n",
        "m\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>12 10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0  0.00 0  0.00 0  0.00 0  0.00 0  0.00 0  0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0  0.00 1 -0.04 1  0.00 0  0.00 0  0.00 0  0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0  0.00 1 -0.04 1 -0.04 0  0.00 0  0.00 0  0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0  0.00 0  0.00 0  0.00 0  0.00 0  0.00 0  0.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.10 0.70 0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.05 0.00 0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.00 0.00 0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.05 0.10 0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.00 0.00 0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.00 0.00 0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.00 0.05 0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.00 0.00 0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.00 0.05 0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.00 0.00 0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0.00 0.00 0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0.05 0.10 0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.00 0.00 0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0.05 0.00 0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0.10 0.70 0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0.05 0.00 0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0.10 0.00 0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0.70 0.10 0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>0.10 0.05 0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.70 0.00 0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>0.10 0.05 0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.70 0.10 0.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>0.10 0.00 0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>0.05 0.00 0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                12 10\n",
              "0   0  0.00 0  0.00 0  0.00 0  0.00 0  0.00 0  0.0...\n",
              "1   0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...\n",
              "2   0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...\n",
              "3   0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...\n",
              "4   0  0.00 1 -0.04 1  0.00 0  0.00 0  0.00 0  0.0...\n",
              "5   0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...\n",
              "6   0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...\n",
              "7   0  0.00 1 -0.04 1 -0.04 0  0.00 0  0.00 0  0.0...\n",
              "8   0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...\n",
              "9   0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...\n",
              "10  0  0.00 1 -0.04 1 -0.04 1 -0.04 1 -0.04 1 -0.0...\n",
              "11  0  0.00 0  0.00 0  0.00 0  0.00 0  0.00 0  0.0...\n",
              "12                                     0.10 0.70 0.10\n",
              "13                                     0.05 0.00 0.05\n",
              "14                                     0.00 0.00 0.00\n",
              "15                                     0.05 0.10 0.70\n",
              "16                                     0.00 0.00 0.10\n",
              "17                                     0.00 0.00 0.05\n",
              "18                                     0.00 0.05 0.10\n",
              "19                                     0.00 0.00 0.70\n",
              "20                                     0.00 0.05 0.10\n",
              "21                                     0.00 0.00 0.05\n",
              "22                                     0.00 0.00 0.10\n",
              "23                                     0.05 0.10 0.70\n",
              "24                                     0.00 0.00 0.00\n",
              "25                                     0.05 0.00 0.05\n",
              "26                                     0.10 0.70 0.10\n",
              "27                                     0.05 0.00 0.00\n",
              "28                                     0.10 0.00 0.00\n",
              "29                                     0.70 0.10 0.05\n",
              "30                                     0.10 0.05 0.00\n",
              "31                                     0.70 0.00 0.00\n",
              "32                                     0.10 0.05 0.00\n",
              "33                                     0.70 0.10 0.05\n",
              "34                                     0.10 0.00 0.00\n",
              "35                                     0.05 0.00 0.00"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjXDlgB2NwaC",
        "colab_type": "code",
        "outputId": "d9c90dbd-7b5f-4b3c-8332-81b21a95a8cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "#Cargar los datos con la función MDFWorld que nos permite separar el espacio en \"tipos\" y \"recompensas\"\n",
        "m = MDPWorld('problem-2.mdp')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-2bf2772c37a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMDPWorld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://raw.githubusercontent.com/riiaa/MDP_and_RL/master/problem-2.mdp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-85bbfa83581f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'N'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'E'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SE'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'S'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SW'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'W'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'NW'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-85bbfa83581f>\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mrows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'https://raw.githubusercontent.com/riiaa/MDP_and_RL/master/problem-2.mdp'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8G04zoXEN6Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m.map.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5Qau1lXCcZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v= m.compute_v(1.animate=TRUE, epsilon=1.e-6)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaAUmMUwCs7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pi=m.random_pi()\n",
        "m.simulate()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}